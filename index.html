
<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <meta name="description" content="ScanNet : Richly-annotated 3D Reconstructions of Indoor Scenes">

    

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ScanNet | Richly-annotated 3D Reconstructions of Indoor Scenes</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="ScanNet" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Richly-annotated 3D Reconstructions of Indoor Scenes" />
<meta property="og:description" content="Richly-annotated 3D Reconstructions of Indoor Scenes" />
<link rel="canonical" href="http://www.scan-net.org/" />
<meta property="og:url" content="http://www.scan-net.org/" />
<meta property="og:site_name" content="ScanNet" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ScanNet" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Richly-annotated 3D Reconstructions of Indoor Scenes","headline":"ScanNet","name":"ScanNet","url":"http://www.scan-net.org/"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=94a40508a6485780aa986fc0d0d9c8b514b1852c">

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header>
          <a class="banner_item" href="#news">News</a>
          <a class="banner_item" href="#introduction">Introduction</a>
          <a class="banner_item" href="#browse">Browse</a>
          <a class="banner_item" href="#code-and-data">Code and Data</a>
          <a class="banner_item" href="/changelog">Changelog</a>
          <a id="forkme_banner" href="https://github.com/ScanNet/ScanNet">View on GitHub</a>

<!--           <h1 id="project_title">ScanNet</h1> -->
<!--           <h2 id="project_tagline">Richly-annotated 3D Reconstructions of Indoor Scenes</h2> -->

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-W2G7SMP1QV', 'auto');
  ga('send', 'pageview');

</script>

<h3 align="center">
ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes<br /><br />
<a href="http://cs.stanford.edu/~adai/publications.html">Angela&nbsp;Dai</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://angelxuanchang.github.io">Angel&nbsp;X.&nbsp;Chang</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://msavva.github.io">Manolis&nbsp;Savva</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.cs.princeton.edu/~mhalber/">Maciej&nbsp;Halber</a><br /><a href="http://www.cs.princeton.edu/~funk/">Thomas&nbsp;Funkhouser</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://graphics.stanford.edu/~niessner/publications.html">Matthias&nbsp;Nie&szlig;ner</a>
</h3>

<h4 align="center">
 Stanford University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Princeton University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Technical University of Munich
</h4>

<p><a href="http://kaldir.vc.in.tum.de/scannet_benchmark"><center>
<img src="img/scannet_benchmark.jpg" /></center></a></p>
<center>Submit to our new ScanNet Benchmark Challenge <a href="http://kaldir.vc.in.tum.de/scannet_benchmark">here</a>!</center>

<p><br />
<a href="http://www.youtube.com/watch?v=Olx4OnoZWQQ">
<img src="img/vid.jpg" alt="ScanNet" style="width:400px; display: block; margin-left: auto; margin-right: auto;" />
</a></p>

<h2 id="news">News</h2>
<ul>
  <li><em>2018-06-11</em> : <strong><a href="changelog#scannet-v2-2018-06-11">ScanNet v2 release</a></strong>.</li>
  <li><em>2018-06-11</em> : <strong><a href="http://kaldir.vc.in.tum.de/scannet_benchmark">ScanNet Benchmark Challenge</a> available</strong>.</li>
  <li><em>2018-02-11</em> : Browsing interface available.</li>
  <li><em>2018-02-04</em> : We are part of the <a href="http://www.robustvision.net/">Robust Vision Challenge 2018 Workshop</a> at <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a> in Salt Lake City.  Participate to test out your algorithms and win prizes!</li>
  <li><em>2017-09-30</em> : Data server code released</li>
  <li><em>2017-02-01</em> : ScanNet initial release</li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>ScanNet is an RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D camera poses, surface reconstructions, and instance-level semantic segmentations.
To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation.
We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval.
More information can be found in our <a href="https://arxiv.org/abs/1702.04405">paper</a>.</p>

<p><br /><br />
<a href="https://arxiv.org/abs/1702.04405">
<img src="img/annotations.png" style="width:640px; display: block; margin-left: auto; margin-right: auto;" />
</a></p>

<p>If you use the ScanNet data or code please cite:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{dai2017scannet,
    title={ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes},
    author={Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Nie{\ss}ner, Matthias},
    booktitle = {Proc. Computer Vision and Pattern Recognition (CVPR), IEEE},
    year = {2017}
}
</code></pre></div></div>

<h2 id="changelog">ChangeLog</h2>
<p>See detailed changelog <a href="changelog">here</a>.</p>
<ul>
  <li><em>2018-11-06</em> : <strong><a href="changelog#scannet-v2-2018-06-11">ScanNet v2 release</a></strong>.</li>
</ul>

<h2 id="license">License</h2>
<p>The ScanNet data is released under the <a href="http://kaldir.vc.in.tum.de/scannet/ScanNet_TOS.pdf">ScanNet Terms of Use</a>, and the code is released under the MIT license.</p>

<h2 id="browse">Browse</h2>
<p>The ScanNet data can be browsed online.</p>
<ul>
  <li><a href="http://kaldir.vc.in.tum.de/scannet_browse/scans/scannet/querier">Query scans</a></li>
  <li><a href="http://kaldir.vc.in.tum.de/scannet_browse/scans/scannet/grouped">Browse scans</a> grouped by scene type</li>
</ul>

<h2 id="code-and-data">Code and Data</h2>
<p>Please visit our main project repository for more information and access to code, data, and trained models: <a href="https://github.com/ScanNet/ScanNet">https://github.com/ScanNet/ScanNet</a></p>

<p><a href="img/voxel-predictions.jpg">
<img src="img/voxel-predictions.jpg" style="width:640px; display: block; margin-left: auto; margin-right: auto;" />
</a></p>
<center><i>Semantic voxel labeling of 3D scans in ScanNet using our 3D CNN architecture. Voxel colors indicate predicted or ground truth
category.</i></center>

<h2 id="acknowledgments">Acknowledgments</h2>
<p>The ScanNet project is funded by Google Tango, Intel, NSF (IIS-1251217 and VEC 1539014/1539099), and a Stanford Graduate fellowship. We also thank Occipital for donating structure sensors and Nvidia for hardware donations, as well as support by the Max-Planck Center for Visual Computing and the Stanford CURIS program.</p>

<p>Toan Vuong, Joseph Chang, and Helen Jiang helped develop the mobile scanning app, and Hope Casey-Allen and Duc Nugyen developed early prototypes of the annotation interfaces.
We thank Alex Sabia for scanning and verifying annotations, and Halle Pollack, Julian Massarani and Michael Fang fo checking annotations.
Last but not least, we would like to thank all the other volunteers who helped with scanning and getting us access to scanning spaces, as well as all the AMT workers who annotated and gave us feedback on the interface design.</p>

<h2><br /></h2>
<p>Last updated: 2018-02-11</p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    
  </body>
</html>
